{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Комментарий от ревьюера:</b> \n",
    "    \n",
    "~~Алена, привет! Меня зовут Влада. Ниже в файле ты найдешь мои комментарии: <font color='green'>зеленый цвет — «все отлично»; </font> <font color='blue'>синий — «хорошо, но можно лучше (исправлять необязательно)»; </font> <font color='red'>красный — «нужно исправить».</font> Комментарии в самом коде я отделяю знаками «###». Пожалуйста, не удаляй мои комментарии, они мне нужны при повторной проверке.~~\n",
    "    \n",
    "Алена, спасибо за доработки, все в порядке!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нулевые значения в 89.8 процентах случаев\n"
     ]
    }
   ],
   "source": [
    "print('нулевые значения в', round(data[data['toxic'] == 0]['toxic'].count()/len(data)*100, 1), 'процентах случаев')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Наблюдается дисбаланс классов.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "~~Уменьшать выборку можно на стадии отладки кода, но в финальном проекте нужно использовать все имеющиеся данные. Как правило, чем больше данных в обучающей выборке, тем качественнее настроенная модель.~~\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процент нетоксичных комментариев остался прежним\n",
    "\n",
    "Напишем функции, которые будут предобрабатывать как тренировочную, так и тестовую выборки:\n",
    "\n",
    "1) Функция clear(text) - оставляет в текстах только латинские символы и пробелы с помощью регулярных выражений, а также приведет все символы к нижнему регистру. \n",
    "2) Функция lemm(sentence) - лемматизирует тексты при помощи  Wordnet Lemmatizer из NLTK с соответствующим POS-тегом. \n",
    "3) Функция stop(sentence)  - избавляется от стоп-слов, то есть слов без смысловой нагрузки с помощью пакета stopwords, который находится в модуле nltk.corpus библиотеки nltk\n",
    "\n",
    "4) Функция total_clean(text) - объединяет все вышеперечисленные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def clear(text):\n",
    "        cleaned = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "        cleaned_words = cleaned.split()\n",
    "        cleaned_sentence = \" \".join(cleaned_words)\n",
    "        cleaned_sentence = cleaned_sentence.lower()\n",
    "        return cleaned_sentence\n",
    "\n",
    "def lemm(sentence):\n",
    "        lemmas = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]\n",
    "        sentence = \" \".join(lemmas)\n",
    "        return sentence\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def stop(sentence):\n",
    "        word_tokens = word_tokenize(sentence) \n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "        final = \" \".join(filtered_sentence)\n",
    "        return final\n",
    "\n",
    "def total_clean(text):\n",
    "    corpus_final = stop(clear(lemm(text)))\n",
    "    return corpus_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Отличная предобработка, молодец :)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Следует сначала проводить очистку текста, а потом лемматизацию. Иначе, например, слово «sentences,» не будет лемматизировано из-за запятой.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все данные были предобработаны и сохранены таким образом: (дабы кривые руки не удалили все что можно)\n",
    "\n",
    "```\n",
    "corpus_cleaned = data.apply(lambda x: total_clean(x['text']), axis = 1)\n",
    "cleaned = pd.DataFrame({'cleaned': corpus_cleaned})\n",
    "cleaned.to_csv('cleaned.csv', index=False)\n",
    "```\n",
    "\n",
    "теперь скачаем очищенный датасет и удалим получившиеся пустыми строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159514 entries, 0 to 159570\n",
      "Data columns (total 3 columns):\n",
      "text            159514 non-null object\n",
      "toxic           159514 non-null int64\n",
      "text_cleaned    159514 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data['text_cleaned'] = pd.read_csv('cleaned.csv')\n",
    "data = data.dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь разделим данные на обучающую, валидационную и тестовую выборки 3:1:1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина: тренировчоной 95708, тестовой 31903, валидационной 31903 выборок\n",
      "нулевые значения в обучающей выборке 89.8 процентах случаев\n"
     ]
    }
   ],
   "source": [
    "train, valid_test = train_test_split(data, test_size = 0.4)\n",
    "valid, test = train_test_split(valid_test, test_size = 0.5)\n",
    "print('Длина: тренировчоной {}, тестовой {}, валидационной {} выборок'.format(len(train), len(test), len(valid)))\n",
    "print('нулевые значения в обучающей выборке', round(train[train['toxic'] == 0]['toxic'].count()/len(train)*100, 1), 'процентах случаев')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как у нас наблюдается сильный дисбаланс классов, удалим из обучающей выборки 40 тыс. нетоксичных комментариев, и повторим токсичные дважды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нулевые значения в обучающей выборке 70.1 процентах случаев\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65483"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nontoxic = train[train['toxic'] == 0].iloc[40000:]\n",
    "toxic = train[train['toxic'] == 1]\n",
    "train = pd.concat([toxic, toxic, nontoxic])\n",
    "print('нулевые значения в обучающей выборке', \n",
    "      round(train[train['toxic'] == 0]['toxic'].count()/len(train)*100, 1), 'процентах случаев')\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Комментарий v2:</b>    \n",
    "Хорошо, дисбаланс на обучающей выборке стал меньше. Почему бы не сделать так, чтобы объектов обоих классов на обучающей выборке стало поровну?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "выделим корпусы обработынных текстов и целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = list(train['text_cleaned'])\n",
    "corpus_valid = list(valid['text_cleaned'])\n",
    "corpus_test = list(test['text_cleaned'])\n",
    "\n",
    "target_train = list(train['toxic'])\n",
    "target_valid = list(valid['toxic'])\n",
    "target_test = list(test['toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве признаков подготовим мешок слов и величины TF-IDF. TF отвечает за количество упоминаний слова в отдельном тексте, а IDF отражает частоту его употребления во всём корпусе. Для обучающей выборки применим методы .fit_transform, для валидационной и тестовой - только .transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#мешок слов\n",
    "count_vect = CountVectorizer()\n",
    "bow_train = count_vect.fit_transform(corpus_train)\n",
    "bow_valid = count_vect.transform(corpus_valid)\n",
    "bow_test = count_vect.transform(corpus_test)\n",
    "\n",
    "#tf-idf\n",
    "count_tf_idf = TfidfVectorizer()\n",
    "tf_idf_train = count_tf_idf.fit_transform(corpus_train)\n",
    "tf_idf_valid = count_tf_idf.transform(corpus_valid)\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Деление выборки на части и подготовка признаков проведены корректно.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С различными признаками данных и гиперпараметрами будем обучать модели Логистической регрессии, Леса и градиентного бустинга CatBoostClassifier. Оценим качество f1 мерой на валидайионной выборке, и для анализа ошибок добавим precision_score и recall_score. Все результаты сохраним в одной таблице"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Для определения токсичности применим величины TF-IDF как признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LogisticRegression', 'tf-idf', \"solver = 'sag', n_jobs = -1, random_state = 123\", 0.7675288251791836, 0.7809131261889664, 0.7545955882352942]\n",
      "['RandomForestClassifier', 'tf-idf', 'random_state = 123, n_estimators = 30', 0.6676465784746041, 0.7149352920601609, 0.6262254901960784]\n",
      "['RandomForestClassifier', 'tf-idf', 'random_state = 123, n_estimators = 60', 0.6720805587136592, 0.7151745592810231, 0.6338848039215687]\n",
      "['RandomForestClassifier', 'tf-idf', 'random_state = 123, n_estimators = 90', 0.6757065838915209, 0.7238361918095905, 0.633578431372549]\n",
      "['CatBoostClassifier', 'tf-idf', \"loss_function='Logloss', iterations=60, depth = 3\", 0.7441708788521115, 0.7953990937608924, 0.6991421568627451]\n",
      "['CatBoostClassifier', 'tf-idf', \"loss_function='Logloss', iterations=60, depth = 5\", 0.7628471113948293, 0.7961359093937375, 0.7322303921568627]\n",
      "['CatBoostClassifier', 'tf-idf', \"loss_function='Logloss', iterations=120, depth = 3\", 0.7524816924328723, 0.802499132245748, 0.7083333333333334]\n",
      "['CatBoostClassifier', 'tf-idf', \"loss_function='Logloss', iterations=120, depth = 5\", 0.7680330473466794, 0.7976897689768977, 0.7405024509803921]\n"
     ]
    }
   ],
   "source": [
    "compare = []\n",
    "columns = ['model', 'feature', 'params', 'f1', 'precision', 'recall']\n",
    "\n",
    "model = LogisticRegression(solver = 'sag', n_jobs = -1, random_state = 123)\n",
    "model.fit(tf_idf_train, target_train)\n",
    "predictions = model.predict(tf_idf_valid)\n",
    "l = ['LogisticRegression', 'tf-idf',\n",
    "                \"solver = 'sag', n_jobs = -1, random_state = 123\", f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "compare.append(l)\n",
    "print(l)\n",
    "\n",
    "for n_estimators in range(30, 91, 30):\n",
    "    model = RandomForestClassifier(random_state = 123, n_estimators = n_estimators)\n",
    "    model.fit(tf_idf_train, target_train)\n",
    "    predictions = model.predict(tf_idf_valid)\n",
    "    l = ['RandomForestClassifier', 'tf-idf',\n",
    "                \"random_state = 123, n_estimators = %d\"%n_estimators, f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "    compare.append(l)\n",
    "    print(l)\n",
    "    \n",
    "for iterations in range(60, 121, 60):\n",
    "    for depth in [3, 5]:\n",
    "            model = CatBoostClassifier(loss_function=\"Logloss\", iterations=iterations, \n",
    "                                       depth = depth, random_state = 123)\n",
    "            model.fit(tf_idf_train, target_train, verbose=False)\n",
    "            predictions = model.predict(tf_idf_valid)\n",
    "            l = ['CatBoostClassifier', 'tf-idf',\n",
    "                \"loss_function='Logloss', iterations={}, depth = {}\".format(\n",
    "                    iterations,depth), f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "            compare.append(l)\n",
    "            print(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Здорово, что рассматриваешь разные модели и настраиваешь их гиперпараметры.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "~~В целом, возможны 2 способа настройки и выбора моделей: \n",
    "1) делим выборку на 2 части – обучающую и тестовую. Для всех моделей на обучающей выборке проводим кросс-валидацию, лучшую модель выбираем по качеству на кросс-валидации. В конце оцениваем качество лучшей модели на тестовой выборке. \n",
    "2) делим выборку на 3 части – обучающую, валидационную, тестовую. На обучающей выборке настраиваем модели, лучшую модель выбираем по качеству на валидационной выборке. Качество лучшей модели оцениваем на тестовой выборке.\n",
    "Качество лучшей модели на валидационной выборке/на кросс-валидации получится оптимистически смещенным, для получения несмещенной оценки качества и нужна тестовая выборка.\n",
    "Так как кросс-валидацию ты не проводишь, то твой вариант – номер 2. Выборку следовало делить не на 2, а на **3 части**.~~\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Для определения токсичности применим мешок слов как признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LogisticRegression', 'bow', \"solver = 'sag', n_jobs = -1, random_state = 123\", 0.6123828545091412, 0.6141756548536209, 0.6106004901960784]\n",
      "['RandomForestClassifier', 'bow', 'random_state = 123, n_estimators = 60', 0.6735059436573848, 0.718804310045186, 0.633578431372549]\n",
      "['RandomForestClassifier', 'bow', 'random_state = 123, n_estimators = 70', 0.67394848386045, 0.7202090592334495, 0.6332720588235294]\n",
      "['RandomForestClassifier', 'bow', 'random_state = 123, n_estimators = 80', 0.6757110166721151, 0.7242466713384723, 0.6332720588235294]\n",
      "['RandomForestClassifier', 'bow', 'random_state = 123, n_estimators = 90', 0.6730044255040156, 0.7236517448008459, 0.6289828431372549]\n",
      "['RandomForestClassifier', 'bow', 'random_state = 123, n_estimators = 100', 0.6765908719123179, 0.7258687258687259, 0.633578431372549]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=60, depth = 3\", 0.7414909513531461, 0.8093512142080463, 0.6841299019607843]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=60, depth = 5\", 0.757580671315064, 0.8046848088184636, 0.7156862745098039]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=80, depth = 3\", 0.75625609359766, 0.8051903114186851, 0.7129289215686274]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=80, depth = 5\", 0.7615681233933161, 0.8006756756756757, 0.7261029411764706]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=100, depth = 3\", 0.7495489585041823, 0.8065654782915637, 0.7000612745098039]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=100, depth = 5\", 0.7584107657801987, 0.7948287441235729, 0.7251838235294118]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=120, depth = 3\", 0.7505743354118805, 0.8081272084805654, 0.7006740196078431]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=120, depth = 5\", 0.7588320696886595, 0.8013628620102214, 0.7205882352941176]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver = 'sag', n_jobs = -1, random_state = 123)\n",
    "model.fit(bow_train, target_train)\n",
    "predictions = model.predict(bow_valid)\n",
    "l = ['LogisticRegression', 'bow',\n",
    "                \"solver = 'sag', n_jobs = -1, random_state = 123\", f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "compare.append(l)\n",
    "print(l)\n",
    "\n",
    "for n_estimators in range(60, 101, 10):\n",
    "    model = RandomForestClassifier(random_state = 123, n_estimators = n_estimators)\n",
    "    model.fit(bow_train, target_train)\n",
    "    predictions = model.predict(bow_valid)\n",
    "    l = ['RandomForestClassifier', 'bow',\n",
    "                \"random_state = 123, n_estimators = %d\"%n_estimators, f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "    compare.append(l)\n",
    "    print(l)\n",
    "    \n",
    "for iterations in range(60, 121, 20):\n",
    "    for depth in [3, 5]:\n",
    "            model = CatBoostClassifier(loss_function=\"Logloss\", iterations=iterations, \n",
    "                                       depth = depth, random_state = 12)\n",
    "            model.fit(bow_train, target_train, verbose=False)\n",
    "            predictions = model.predict(bow_valid)\n",
    "            l = ['CatBoostClassifier', 'bow',\n",
    "                \"loss_function='Logloss', iterations={}, depth = {}\".format(\n",
    "                    iterations,depth), f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "            compare.append(l)\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>feature</th>\n",
       "      <th>params</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>loss_function='Logloss', iterations=120, depth...</td>\n",
       "      <td>0.768033</td>\n",
       "      <td>0.797690</td>\n",
       "      <td>0.740502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>solver = 'sag', n_jobs = -1, random_state = 123</td>\n",
       "      <td>0.767529</td>\n",
       "      <td>0.780913</td>\n",
       "      <td>0.754596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>loss_function='Logloss', iterations=60, depth = 5</td>\n",
       "      <td>0.762847</td>\n",
       "      <td>0.796136</td>\n",
       "      <td>0.732230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=80, depth = 5</td>\n",
       "      <td>0.761568</td>\n",
       "      <td>0.800676</td>\n",
       "      <td>0.726103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=120, depth...</td>\n",
       "      <td>0.758832</td>\n",
       "      <td>0.801363</td>\n",
       "      <td>0.720588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=100, depth...</td>\n",
       "      <td>0.758411</td>\n",
       "      <td>0.794829</td>\n",
       "      <td>0.725184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=60, depth = 5</td>\n",
       "      <td>0.757581</td>\n",
       "      <td>0.804685</td>\n",
       "      <td>0.715686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=80, depth = 3</td>\n",
       "      <td>0.756256</td>\n",
       "      <td>0.805190</td>\n",
       "      <td>0.712929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>loss_function='Logloss', iterations=120, depth...</td>\n",
       "      <td>0.752482</td>\n",
       "      <td>0.802499</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=120, depth...</td>\n",
       "      <td>0.750574</td>\n",
       "      <td>0.808127</td>\n",
       "      <td>0.700674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=100, depth...</td>\n",
       "      <td>0.749549</td>\n",
       "      <td>0.806565</td>\n",
       "      <td>0.700061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>loss_function='Logloss', iterations=60, depth = 3</td>\n",
       "      <td>0.744171</td>\n",
       "      <td>0.795399</td>\n",
       "      <td>0.699142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=60, depth = 3</td>\n",
       "      <td>0.741491</td>\n",
       "      <td>0.809351</td>\n",
       "      <td>0.684130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>random_state = 123, n_estimators = 100</td>\n",
       "      <td>0.676591</td>\n",
       "      <td>0.725869</td>\n",
       "      <td>0.633578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>random_state = 123, n_estimators = 80</td>\n",
       "      <td>0.675711</td>\n",
       "      <td>0.724247</td>\n",
       "      <td>0.633272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>random_state = 123, n_estimators = 90</td>\n",
       "      <td>0.675707</td>\n",
       "      <td>0.723836</td>\n",
       "      <td>0.633578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>random_state = 123, n_estimators = 70</td>\n",
       "      <td>0.673948</td>\n",
       "      <td>0.720209</td>\n",
       "      <td>0.633272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>random_state = 123, n_estimators = 60</td>\n",
       "      <td>0.673506</td>\n",
       "      <td>0.718804</td>\n",
       "      <td>0.633578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>random_state = 123, n_estimators = 90</td>\n",
       "      <td>0.673004</td>\n",
       "      <td>0.723652</td>\n",
       "      <td>0.628983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>random_state = 123, n_estimators = 60</td>\n",
       "      <td>0.672081</td>\n",
       "      <td>0.715175</td>\n",
       "      <td>0.633885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>random_state = 123, n_estimators = 30</td>\n",
       "      <td>0.667647</td>\n",
       "      <td>0.714935</td>\n",
       "      <td>0.626225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>bow</td>\n",
       "      <td>solver = 'sag', n_jobs = -1, random_state = 123</td>\n",
       "      <td>0.612383</td>\n",
       "      <td>0.614176</td>\n",
       "      <td>0.610600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model feature  \\\n",
       "7       CatBoostClassifier  tf-idf   \n",
       "0       LogisticRegression  tf-idf   \n",
       "5       CatBoostClassifier  tf-idf   \n",
       "17      CatBoostClassifier     bow   \n",
       "21      CatBoostClassifier     bow   \n",
       "19      CatBoostClassifier     bow   \n",
       "15      CatBoostClassifier     bow   \n",
       "16      CatBoostClassifier     bow   \n",
       "6       CatBoostClassifier  tf-idf   \n",
       "20      CatBoostClassifier     bow   \n",
       "18      CatBoostClassifier     bow   \n",
       "4       CatBoostClassifier  tf-idf   \n",
       "14      CatBoostClassifier     bow   \n",
       "13  RandomForestClassifier     bow   \n",
       "11  RandomForestClassifier     bow   \n",
       "3   RandomForestClassifier  tf-idf   \n",
       "10  RandomForestClassifier     bow   \n",
       "9   RandomForestClassifier     bow   \n",
       "12  RandomForestClassifier     bow   \n",
       "2   RandomForestClassifier  tf-idf   \n",
       "1   RandomForestClassifier  tf-idf   \n",
       "8       LogisticRegression     bow   \n",
       "\n",
       "                                               params        f1  precision  \\\n",
       "7   loss_function='Logloss', iterations=120, depth...  0.768033   0.797690   \n",
       "0     solver = 'sag', n_jobs = -1, random_state = 123  0.767529   0.780913   \n",
       "5   loss_function='Logloss', iterations=60, depth = 5  0.762847   0.796136   \n",
       "17  loss_function='Logloss', iterations=80, depth = 5  0.761568   0.800676   \n",
       "21  loss_function='Logloss', iterations=120, depth...  0.758832   0.801363   \n",
       "19  loss_function='Logloss', iterations=100, depth...  0.758411   0.794829   \n",
       "15  loss_function='Logloss', iterations=60, depth = 5  0.757581   0.804685   \n",
       "16  loss_function='Logloss', iterations=80, depth = 3  0.756256   0.805190   \n",
       "6   loss_function='Logloss', iterations=120, depth...  0.752482   0.802499   \n",
       "20  loss_function='Logloss', iterations=120, depth...  0.750574   0.808127   \n",
       "18  loss_function='Logloss', iterations=100, depth...  0.749549   0.806565   \n",
       "4   loss_function='Logloss', iterations=60, depth = 3  0.744171   0.795399   \n",
       "14  loss_function='Logloss', iterations=60, depth = 3  0.741491   0.809351   \n",
       "13             random_state = 123, n_estimators = 100  0.676591   0.725869   \n",
       "11              random_state = 123, n_estimators = 80  0.675711   0.724247   \n",
       "3               random_state = 123, n_estimators = 90  0.675707   0.723836   \n",
       "10              random_state = 123, n_estimators = 70  0.673948   0.720209   \n",
       "9               random_state = 123, n_estimators = 60  0.673506   0.718804   \n",
       "12              random_state = 123, n_estimators = 90  0.673004   0.723652   \n",
       "2               random_state = 123, n_estimators = 60  0.672081   0.715175   \n",
       "1               random_state = 123, n_estimators = 30  0.667647   0.714935   \n",
       "8     solver = 'sag', n_jobs = -1, random_state = 123  0.612383   0.614176   \n",
       "\n",
       "      recall  \n",
       "7   0.740502  \n",
       "0   0.754596  \n",
       "5   0.732230  \n",
       "17  0.726103  \n",
       "21  0.720588  \n",
       "19  0.725184  \n",
       "15  0.715686  \n",
       "16  0.712929  \n",
       "6   0.708333  \n",
       "20  0.700674  \n",
       "18  0.700061  \n",
       "4   0.699142  \n",
       "14  0.684130  \n",
       "13  0.633578  \n",
       "11  0.633272  \n",
       "3   0.633578  \n",
       "10  0.633272  \n",
       "9   0.633578  \n",
       "12  0.628983  \n",
       "2   0.633885  \n",
       "1   0.626225  \n",
       "8   0.610600  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compared_1 = pd.DataFrame(data = compare, columns = columns)\n",
    "compared_1.sort_values('f1', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Таблица наглядная.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "~~Как я писала выше, работать следует со всеми данными.~~\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CatBoostClassifier', 'tf-idf', \"loss_function='Logloss', iterations=120, depth = 5\", 0.7680330473466794, 0.7976897689768977, 0.7405024509803921]\n"
     ]
    }
   ],
   "source": [
    "print(list(compared_1.iloc[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всего на валидационной выборке себя показала модель CatBoostClassifier(loss_function='Logloss', iterations=120, depth = 5, random_state = 123), обученная на tf-idf. Проверим ее на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.361858\n",
      "0:\tlearn: 0.5447649\ttotal: 1.73s\tremaining: 3m 25s\n",
      "30:\tlearn: 0.3018870\ttotal: 56.1s\tremaining: 2m 41s\n",
      "60:\tlearn: 0.2582880\ttotal: 1m 49s\tremaining: 1m 45s\n",
      "90:\tlearn: 0.2328546\ttotal: 2m 42s\tremaining: 51.9s\n",
      "119:\tlearn: 0.2177634\ttotal: 3m 34s\tremaining: 0us\n",
      "f1: 0.7529834886382213 , precision: 0.7857386557488911 , recall: 0.7228499686126805\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(loss_function=\"Logloss\", iterations=120, depth = 5, random_state = 123)\n",
    "model.fit(tf_idf_train, target_train, verbose=30)\n",
    "predictions = model.predict(tf_idf_test)\n",
    "print('f1:',f1_score(target_test, predictions), ', precision:',precision_score(target_test, predictions),\n",
    "      ', recall:', recall_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель также справилась с тестовой выборкой и показала f1 = 0.75, причем точность и полнота получились близки друг к другу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Комментарий v2:</b>    \n",
    "Отлично, требуемое качество достигнуто.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения данной задачи нам хватило мешка слов и tf-idf, причем модели, обученные на первых и вторых видах признаков показали себя примерно одинаково. Мешок слов учитывает частоту употребления слов. TF-IDF показывает, как часто уникальное слово встречается во всём корпусе и в отдельном его тексте. И этого оказалось достаточно, так как токсичность комментария вполне можно определить по наличию или частоте употребления определенных слов, Embeddings и Bert не понадобились.\n",
    "\n",
    "Для более сложных задач этого, скорее всего не хватит, так как мешок слов и TF-IDF не умеют учитывать смысл, контекст и свойства слов при переводе в векторы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Итоговый комментарий:</b> \n",
    "\n",
    "~~Спасибо, ты провела отличное исследование, осталось его немного доработать: использовать все имеющиеся данные; поделить выборку на 3 части.~~\n",
    "    \n",
    "Все отлично, молодец! :)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
