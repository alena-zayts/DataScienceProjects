{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оглавление\n",
    "* [Описание проекта](#b0)\n",
    "* [1. Загрузка и подготовка данных](#b1)\n",
    "* [2. Выбор и обучение модели](#b2)\n",
    "* [3. Выводы](#b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание проекта<a class=\"anchor\" id=\"b0\"></a>\n",
    "\n",
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Необходимо на наборе данных с разметкой о токсичности правок обучить модель классифицировать комментарии на позитивные и негативные. \n",
    "\n",
    "Значение метрики качества F1 должно быть не меньше 0.75. \n",
    "\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле data/toxic_comments.csv. Столбец text в нём содержит текст комментария, а toxic — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\alena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\alena\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загрузка и подготовка данных<a class=\"anchor\" id=\"b1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/toxic_comments.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нулевые значения в 89.8 процентах случаев\n"
     ]
    }
   ],
   "source": [
    "print('нулевые значения в', round(data[data['toxic'] == 0]['toxic'].count()/len(data)*100, 1), 'процентах случаев')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдается дисбаланс классов.\n",
    "\n",
    "Напишем функции обработки текстов, которые будут предобрабатывать как тренировочную, так и тестовую выборки:\n",
    "\n",
    "1) Функция clear(text) - оставляет в текстах только латинские символы и пробелы с помощью регулярных выражений, а также приводит все символы к нижнему регистру. \n",
    "2) Функция lemm(sentence) - лемматизирует тексты при помощи  Wordnet Lemmatizer из NLTK с соответствующим POS-тегом. \n",
    "3) Функция stop(sentence)  - избавляется от стоп-слов, то есть слов без смысловой нагрузки с помощью пакета stopwords, который находится в модуле nltk.corpus библиотеки nltk\n",
    "\n",
    "4) Функция total_clean(text) - объединяет все вышеперечисленные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79799fe98c3044a78eda5f5d22c08a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=159292)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def clear(text):\n",
    "        cleaned = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "        cleaned_words = cleaned.split()\n",
    "        cleaned_sentence = \" \".join(cleaned_words)\n",
    "        cleaned_sentence = cleaned_sentence.lower()\n",
    "        return cleaned_sentence\n",
    "\n",
    "def lemm(sentence):\n",
    "        lemmas = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]\n",
    "        sentence = \" \".join(lemmas)\n",
    "        return sentence\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def stop(sentence):\n",
    "        word_tokens = word_tokenize(sentence) \n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "        final = \" \".join(filtered_sentence)\n",
    "        return final\n",
    "\n",
    "f = IntProgress(min=0, max=len(data))\n",
    "display(f)\n",
    "\n",
    "def total_clean(row):\n",
    "    global f\n",
    "    f.value += 1\n",
    "    i = row._name\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    text = row['text']\n",
    "    corpus_final = stop(clear(lemm(text)))\n",
    "    return corpus_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все данные были предобработаны и сохранены (так как считается долго) таким образом: \n",
    "\n",
    "```\n",
    "corpus_cleaned = data.apply(lambda x: total_clean(x['text']), axis = 1)\n",
    "cleaned = pd.DataFrame({'cleaned': corpus_cleaned})\n",
    "cleaned.to_csv('cleaned.csv', index=False)\n",
    "```\n",
    "\n",
    "Скачаем очищенный датасет и удалим получившиеся пустыми строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159231 entries, 0 to 159291\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   Unnamed: 0    159231 non-null  int64 \n",
      " 1   text          159231 non-null  object\n",
      " 2   toxic         159231 non-null  int64 \n",
      " 3   text_cleaned  159231 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data['text_cleaned'] = pd.read_csv('data/cleaned.csv')\n",
    "data = data.dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую, валидационную и тестовую выборки в отношеннии 3:1:1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина: тренировчоной 95538, тестовой 31847, валидационной 31846 выборок\n",
      "нулевые значения в обучающей выборке 89.9 процентах случаев\n"
     ]
    }
   ],
   "source": [
    "train, valid_test = train_test_split(data, test_size = 0.4)\n",
    "valid, test = train_test_split(valid_test, test_size = 0.5)\n",
    "print('Длина: тренировчоной {}, тестовой {}, валидационной {} выборок'.format(len(train), len(test), len(valid)))\n",
    "print('нулевые значения в обучающей выборке', round(train[train['toxic'] == 0]['toxic'].count()/len(train)*100, 1), 'процентах случаев')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как у нас наблюдается сильный дисбаланс классов, удалим из обучающей выборки 40 тыс. нетоксичных комментариев, и повторим токсичные дважды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нулевые значения в обучающей выборке 70.4 процентах случаев\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65192"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nontoxic = train[train['toxic'] == 0].iloc[40000:]\n",
    "toxic = train[train['toxic'] == 1]\n",
    "train = pd.concat([toxic, toxic, nontoxic])\n",
    "print('нулевые значения в обучающей выборке', \n",
    "      round(train[train['toxic'] == 0]['toxic'].count()/len(train)*100, 1), 'процентах случаев')\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим корпусы обработынных текстов и целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = list(train['text_cleaned'])\n",
    "corpus_valid = list(valid['text_cleaned'])\n",
    "corpus_test = list(test['text_cleaned'])\n",
    "\n",
    "target_train = list(train['toxic'])\n",
    "target_valid = list(valid['toxic'])\n",
    "target_test = list(test['toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве признаков подготовим мешок слов и величины TF-IDF. TF отвечает за количество упоминаний слова в отдельном тексте, а IDF отражает частоту его употребления во всём корпусе. Для обучающей выборки применим методы .fit_transform, для валидационной и тестовой - только .transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#мешок слов\n",
    "count_vect = CountVectorizer()\n",
    "bow_train = count_vect.fit_transform(corpus_train)\n",
    "bow_valid = count_vect.transform(corpus_valid)\n",
    "bow_test = count_vect.transform(corpus_test)\n",
    "\n",
    "#tf-idf\n",
    "count_tf_idf = TfidfVectorizer()\n",
    "tf_idf_train = count_tf_idf.fit_transform(corpus_train)\n",
    "tf_idf_valid = count_tf_idf.transform(corpus_valid)\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Выбор и обучение модели<a class=\"anchor\" id=\"b2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С различными признаками данных и гиперпараметрами будем обучать модели Логистической регрессии, Леса и градиентного бустинга CatBoostClassifier. Оценим качество f1 мерой на валидайионной выборке, и для анализа ошибок добавим precision_score и recall_score. Все результаты сохраним в одной таблице"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Для определения токсичности применим величины TF-IDF как признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LogisticRegression', 'tf-idf', \"solver = 'sag', n_jobs = -1, random_state = 123\", 0.7632608355499922, 0.778984350047908, 0.748159509202454]\n",
      "['RandomForestClassifier', 'tf-idf', 'random_state = 123, n_estimators = 30', 0.6677869265669637, 0.7383872166480863, 0.6095092024539878]\n",
      "['RandomForestClassifier', 'tf-idf', 'random_state = 123, n_estimators = 60', 0.6829594911282223, 0.7516580692704495, 0.6257668711656442]\n",
      "['RandomForestClassifier', 'tf-idf', 'random_state = 123, n_estimators = 90', 0.6845277963831212, 0.7536873156342183, 0.6269938650306749]\n",
      "['CatBoostClassifier', 'tf-idf', \"loss_function='Logloss', iterations=60, depth = 3\", 0.7383100902378997, 0.7936507936507936, 0.6901840490797546]\n",
      "['CatBoostClassifier', 'tf-idf', \"loss_function='Logloss', iterations=60, depth = 5\", 0.7504005126561999, 0.7853789403085177, 0.7184049079754601]\n",
      "['CatBoostClassifier', 'tf-idf', \"loss_function='Logloss', iterations=120, depth = 3\", 0.7510548523206751, 0.7973811164713991, 0.7098159509202454]\n",
      "['CatBoostClassifier', 'tf-idf', \"loss_function='Logloss', iterations=120, depth = 5\", 0.7555837161412957, 0.7811988208319686, 0.7315950920245399]\n"
     ]
    }
   ],
   "source": [
    "compare = []\n",
    "columns = ['model', 'feature', 'params', 'f1', 'precision', 'recall']\n",
    "\n",
    "model = LogisticRegression(solver = 'sag', n_jobs = -1, random_state = 123)\n",
    "model.fit(tf_idf_train, target_train)\n",
    "predictions = model.predict(tf_idf_valid)\n",
    "l = ['LogisticRegression', 'tf-idf',\n",
    "                \"solver = 'sag', n_jobs = -1, random_state = 123\", f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "compare.append(l)\n",
    "print(l)\n",
    "\n",
    "for n_estimators in range(30, 91, 30):\n",
    "    model = RandomForestClassifier(random_state = 123, n_estimators = n_estimators)\n",
    "    model.fit(tf_idf_train, target_train)\n",
    "    predictions = model.predict(tf_idf_valid)\n",
    "    l = ['RandomForestClassifier', 'tf-idf',\n",
    "                \"random_state = 123, n_estimators = %d\"%n_estimators, f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "    compare.append(l)\n",
    "    print(l)\n",
    "    \n",
    "for iterations in range(60, 121, 60):\n",
    "    for depth in [3, 5]:\n",
    "            model = CatBoostClassifier(loss_function=\"Logloss\", iterations=iterations, \n",
    "                                       depth = depth, random_state = 123)\n",
    "            model.fit(tf_idf_train, target_train, verbose=False)\n",
    "            predictions = model.predict(tf_idf_valid)\n",
    "            l = ['CatBoostClassifier', 'tf-idf',\n",
    "                \"loss_function='Logloss', iterations={}, depth = {}\".format(\n",
    "                    iterations,depth), f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "            compare.append(l)\n",
    "            print(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Для определения токсичности применим мешок слов как признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alena\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LogisticRegression', 'bow', \"solver = 'sag', n_jobs = -1, random_state = 123\", 0.6013870541611626, 0.6512875536480687, 0.5585889570552147]\n",
      "['RandomForestClassifier', 'bow', 'random_state = 123, n_estimators = 60', 0.6778367617783675, 0.7380780346820809, 0.6266871165644172]\n",
      "['RandomForestClassifier', 'bow', 'random_state = 123, n_estimators = 70', 0.6791639017916391, 0.7395231213872833, 0.6279141104294479]\n",
      "['RandomForestClassifier', 'bow', 'random_state = 123, n_estimators = 80', 0.6763877381938692, 0.7354954954954955, 0.6260736196319019]\n",
      "['RandomForestClassifier', 'bow', 'random_state = 123, n_estimators = 90', 0.680780681442276, 0.7386934673366834, 0.6312883435582822]\n",
      "['RandomForestClassifier', 'bow', 'random_state = 123, n_estimators = 100', 0.6819758797290599, 0.7389903329752954, 0.6331288343558282]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=60, depth = 3\", 0.7370675453047776, 0.796085409252669, 0.6861963190184049]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=60, depth = 5\", 0.7452001301659615, 0.7934857934857935, 0.7024539877300614]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=80, depth = 3\", 0.7387328543435664, 0.789804469273743, 0.6938650306748466]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=80, depth = 5\", 0.7478625584771738, 0.7887036406941137, 0.7110429447852761]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=100, depth = 3\", 0.7478542510121458, 0.7921097770154374, 0.7082822085889571]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=100, depth = 5\", 0.7576290142195239, 0.7905968656218739, 0.7273006134969325]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=120, depth = 3\", 0.7423152387181163, 0.7948179271708683, 0.696319018404908]\n",
      "['CatBoostClassifier', 'bow', \"loss_function='Logloss', iterations=120, depth = 5\", 0.7506014434643143, 0.7865546218487395, 0.7177914110429447]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver = 'sag', n_jobs = -1, random_state = 123)\n",
    "model.fit(bow_train, target_train)\n",
    "predictions = model.predict(bow_valid)\n",
    "l = ['LogisticRegression', 'bow',\n",
    "                \"solver = 'sag', n_jobs = -1, random_state = 123\", f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "compare.append(l)\n",
    "print(l)\n",
    "\n",
    "for n_estimators in range(60, 101, 10):\n",
    "    model = RandomForestClassifier(random_state = 123, n_estimators = n_estimators)\n",
    "    model.fit(bow_train, target_train)\n",
    "    predictions = model.predict(bow_valid)\n",
    "    l = ['RandomForestClassifier', 'bow',\n",
    "                \"random_state = 123, n_estimators = %d\"%n_estimators, f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "    compare.append(l)\n",
    "    print(l)\n",
    "    \n",
    "for iterations in range(60, 121, 20):\n",
    "    for depth in [3, 5]:\n",
    "            model = CatBoostClassifier(loss_function=\"Logloss\", iterations=iterations, \n",
    "                                       depth = depth, random_state = 12)\n",
    "            model.fit(bow_train, target_train, verbose=False)\n",
    "            predictions = model.predict(bow_valid)\n",
    "            l = ['CatBoostClassifier', 'bow',\n",
    "                \"loss_function='Logloss', iterations={}, depth = {}\".format(\n",
    "                    iterations,depth), f1_score(target_valid, predictions), \n",
    "               precision_score(target_valid, predictions), recall_score(target_valid, predictions)]\n",
    "            compare.append(l)\n",
    "            print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>feature</th>\n",
       "      <th>params</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>solver = 'sag', n_jobs = -1, random_state = 123</td>\n",
       "      <td>0.763261</td>\n",
       "      <td>0.778984</td>\n",
       "      <td>0.748160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=100, depth...</td>\n",
       "      <td>0.757629</td>\n",
       "      <td>0.790597</td>\n",
       "      <td>0.727301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>loss_function='Logloss', iterations=120, depth...</td>\n",
       "      <td>0.755584</td>\n",
       "      <td>0.781199</td>\n",
       "      <td>0.731595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>loss_function='Logloss', iterations=120, depth...</td>\n",
       "      <td>0.751055</td>\n",
       "      <td>0.797381</td>\n",
       "      <td>0.709816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=120, depth...</td>\n",
       "      <td>0.750601</td>\n",
       "      <td>0.786555</td>\n",
       "      <td>0.717791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>loss_function='Logloss', iterations=60, depth = 5</td>\n",
       "      <td>0.750401</td>\n",
       "      <td>0.785379</td>\n",
       "      <td>0.718405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=80, depth = 5</td>\n",
       "      <td>0.747863</td>\n",
       "      <td>0.788704</td>\n",
       "      <td>0.711043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=100, depth...</td>\n",
       "      <td>0.747854</td>\n",
       "      <td>0.792110</td>\n",
       "      <td>0.708282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=60, depth = 5</td>\n",
       "      <td>0.745200</td>\n",
       "      <td>0.793486</td>\n",
       "      <td>0.702454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=120, depth...</td>\n",
       "      <td>0.742315</td>\n",
       "      <td>0.794818</td>\n",
       "      <td>0.696319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=80, depth = 3</td>\n",
       "      <td>0.738733</td>\n",
       "      <td>0.789804</td>\n",
       "      <td>0.693865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>loss_function='Logloss', iterations=60, depth = 3</td>\n",
       "      <td>0.738310</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.690184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>loss_function='Logloss', iterations=60, depth = 3</td>\n",
       "      <td>0.737068</td>\n",
       "      <td>0.796085</td>\n",
       "      <td>0.686196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>random_state = 123, n_estimators = 90</td>\n",
       "      <td>0.684528</td>\n",
       "      <td>0.753687</td>\n",
       "      <td>0.626994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>random_state = 123, n_estimators = 60</td>\n",
       "      <td>0.682959</td>\n",
       "      <td>0.751658</td>\n",
       "      <td>0.625767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>random_state = 123, n_estimators = 100</td>\n",
       "      <td>0.681976</td>\n",
       "      <td>0.738990</td>\n",
       "      <td>0.633129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>random_state = 123, n_estimators = 90</td>\n",
       "      <td>0.680781</td>\n",
       "      <td>0.738693</td>\n",
       "      <td>0.631288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>random_state = 123, n_estimators = 70</td>\n",
       "      <td>0.679164</td>\n",
       "      <td>0.739523</td>\n",
       "      <td>0.627914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>random_state = 123, n_estimators = 60</td>\n",
       "      <td>0.677837</td>\n",
       "      <td>0.738078</td>\n",
       "      <td>0.626687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>bow</td>\n",
       "      <td>random_state = 123, n_estimators = 80</td>\n",
       "      <td>0.676388</td>\n",
       "      <td>0.735495</td>\n",
       "      <td>0.626074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>tf-idf</td>\n",
       "      <td>random_state = 123, n_estimators = 30</td>\n",
       "      <td>0.667787</td>\n",
       "      <td>0.738387</td>\n",
       "      <td>0.609509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>bow</td>\n",
       "      <td>solver = 'sag', n_jobs = -1, random_state = 123</td>\n",
       "      <td>0.601387</td>\n",
       "      <td>0.651288</td>\n",
       "      <td>0.558589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model feature  \\\n",
       "0       LogisticRegression  tf-idf   \n",
       "19      CatBoostClassifier     bow   \n",
       "7       CatBoostClassifier  tf-idf   \n",
       "6       CatBoostClassifier  tf-idf   \n",
       "21      CatBoostClassifier     bow   \n",
       "5       CatBoostClassifier  tf-idf   \n",
       "17      CatBoostClassifier     bow   \n",
       "18      CatBoostClassifier     bow   \n",
       "15      CatBoostClassifier     bow   \n",
       "20      CatBoostClassifier     bow   \n",
       "16      CatBoostClassifier     bow   \n",
       "4       CatBoostClassifier  tf-idf   \n",
       "14      CatBoostClassifier     bow   \n",
       "3   RandomForestClassifier  tf-idf   \n",
       "2   RandomForestClassifier  tf-idf   \n",
       "13  RandomForestClassifier     bow   \n",
       "12  RandomForestClassifier     bow   \n",
       "10  RandomForestClassifier     bow   \n",
       "9   RandomForestClassifier     bow   \n",
       "11  RandomForestClassifier     bow   \n",
       "1   RandomForestClassifier  tf-idf   \n",
       "8       LogisticRegression     bow   \n",
       "\n",
       "                                               params        f1  precision  \\\n",
       "0     solver = 'sag', n_jobs = -1, random_state = 123  0.763261   0.778984   \n",
       "19  loss_function='Logloss', iterations=100, depth...  0.757629   0.790597   \n",
       "7   loss_function='Logloss', iterations=120, depth...  0.755584   0.781199   \n",
       "6   loss_function='Logloss', iterations=120, depth...  0.751055   0.797381   \n",
       "21  loss_function='Logloss', iterations=120, depth...  0.750601   0.786555   \n",
       "5   loss_function='Logloss', iterations=60, depth = 5  0.750401   0.785379   \n",
       "17  loss_function='Logloss', iterations=80, depth = 5  0.747863   0.788704   \n",
       "18  loss_function='Logloss', iterations=100, depth...  0.747854   0.792110   \n",
       "15  loss_function='Logloss', iterations=60, depth = 5  0.745200   0.793486   \n",
       "20  loss_function='Logloss', iterations=120, depth...  0.742315   0.794818   \n",
       "16  loss_function='Logloss', iterations=80, depth = 3  0.738733   0.789804   \n",
       "4   loss_function='Logloss', iterations=60, depth = 3  0.738310   0.793651   \n",
       "14  loss_function='Logloss', iterations=60, depth = 3  0.737068   0.796085   \n",
       "3               random_state = 123, n_estimators = 90  0.684528   0.753687   \n",
       "2               random_state = 123, n_estimators = 60  0.682959   0.751658   \n",
       "13             random_state = 123, n_estimators = 100  0.681976   0.738990   \n",
       "12              random_state = 123, n_estimators = 90  0.680781   0.738693   \n",
       "10              random_state = 123, n_estimators = 70  0.679164   0.739523   \n",
       "9               random_state = 123, n_estimators = 60  0.677837   0.738078   \n",
       "11              random_state = 123, n_estimators = 80  0.676388   0.735495   \n",
       "1               random_state = 123, n_estimators = 30  0.667787   0.738387   \n",
       "8     solver = 'sag', n_jobs = -1, random_state = 123  0.601387   0.651288   \n",
       "\n",
       "      recall  \n",
       "0   0.748160  \n",
       "19  0.727301  \n",
       "7   0.731595  \n",
       "6   0.709816  \n",
       "21  0.717791  \n",
       "5   0.718405  \n",
       "17  0.711043  \n",
       "18  0.708282  \n",
       "15  0.702454  \n",
       "20  0.696319  \n",
       "16  0.693865  \n",
       "4   0.690184  \n",
       "14  0.686196  \n",
       "3   0.626994  \n",
       "2   0.625767  \n",
       "13  0.633129  \n",
       "12  0.631288  \n",
       "10  0.627914  \n",
       "9   0.626687  \n",
       "11  0.626074  \n",
       "1   0.609509  \n",
       "8   0.558589  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compared_1 = pd.DataFrame(data = compare, columns = columns)\n",
    "compared_1.sort_values('f1', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model feature  \\\n",
      "0  LogisticRegression  tf-idf   \n",
      "\n",
      "                                            params        f1  precision  \\\n",
      "0  solver = 'sag', n_jobs = -1, random_state = 123  0.763261   0.778984   \n",
      "\n",
      "    recall  \n",
      "0  0.74816  \n"
     ]
    }
   ],
   "source": [
    "print(compared_1.sort_values('f1', ascending = False).head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всего на валидационной выборке себя показала модель LogisticRegression(solver='sag', n_jobs=-1, random_state=123), обученная на tf-idf. Проверим ее на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.7592332865825151 , precision: 0.7743165924984107 , recall: 0.7447263833690003\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='sag', n_jobs=-1, random_state=123)\n",
    "model.fit(tf_idf_train, target_train)\n",
    "predictions = model.predict(tf_idf_test)\n",
    "print('f1:',f1_score(target_test, predictions), ', precision:',precision_score(target_test, predictions),\n",
    "      ', recall:', recall_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель также справилась с тестовой выборкой и показала f1=0.76, причем точность и полнота получились близки друг к другу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы<a class=\"anchor\" id=\"b3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения данной задачи нам хватило мешка слов и tf-idf, причем модели, обученные на первых и вторых видах признаков показали себя примерно одинаково. Мешок слов учитывает частоту употребления слов. TF-IDF показывает, как часто уникальное слово встречается во всём корпусе и в отдельном его тексте. И этого оказалось достаточно, так как токсичность комментария вполне можно определить по наличию или частоте употребления определенных слов, Embeddings и Bert не понадобились.\n",
    "\n",
    "Для более сложных задач этого, скорее всего не хватит, так как мешок слов и TF-IDF не умеют учитывать смысл, контекст и свойства слов при переводе в векторы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
